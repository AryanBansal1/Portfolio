{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc1c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Libraries\n",
    "from pypdf import PdfReader\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from google import genai\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce5436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting text\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "\n",
    "#Creating a list\n",
    "doc =[]\n",
    "\n",
    "#Extracting text from pdf splitting it and storing it in list\n",
    "reader = PdfReader(\"test_pdf.pdf\")\n",
    "i=1\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    chunks=splitter.split_text(text)\n",
    "    doc.extend([{\"page_number\":i, \"content\":chunk}for chunk in chunks])\n",
    "    i=i+1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c16222ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming data\n",
    "document =[d[\"content\"] for d in doc]\n",
    "metadata =[ {\"page_number\":d[\"page_number\"]}  for d in doc]\n",
    "ids=[f\"{i}\" for i in range(len(doc))]\n",
    "\n",
    "#Storing in VectorDB\n",
    "client = chromadb.PersistentClient()\n",
    "collection = client.get_or_create_collection(\"my_collection\")\n",
    "collection.add(\n",
    "    documents=document,\n",
    "    metadatas=metadata,\n",
    "    ids=ids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1dcc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the main architecture proposed in the paper is **The Transformer**, which utilizes **stacked self-attention** mechanisms and **point-wise, fully connected feed-forward networks**.\n",
      "\n",
      "In essence, the paper is introducing or elaborating on the Transformer model architecture, and Figure 1 illustrates this architecture. The key components highlighted are:\n",
      "\n",
      "*   **Stacked self-attention:** This implies that the model uses multiple layers of self-attention, allowing it to capture complex relationships within the input sequence.\n",
      "*   **Point-wise, fully connected layers:** After the self-attention layers, the model uses feed-forward networks applied to each position in the sequence independently.\n",
      "\n",
      "Therefore, the main architecture is the Transformer, characterized by its reliance on stacked self-attention and point-wise fully connected layers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question from ChromaDB\n",
    "query = \"What is the main architecture proposed in this paper?\"\n",
    "result = collection.query(\n",
    "    n_results=1,\n",
    "    query_texts=[query]\n",
    ")\n",
    "\n",
    "# Calling Gemini and asking it \n",
    "gemini = genai.Client(api_key=os.getenv(\"gemini_key\"))\n",
    "ai_result = gemini.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=f\"Explain {query} in context to {result['documents']}\"\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f710d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the main architecture proposed in the paper is **The Transformer**, which utilizes **stacked self-attention** mechanisms and **point-wise, fully connected feed-forward networks**.\n",
      "\n",
      "In essence, the paper is introducing or elaborating on the Transformer model architecture, and Figure 1 illustrates this architecture. The key components highlighted are:\n",
      "\n",
      "*   **Stacked self-attention:** This implies that the model uses multiple layers of self-attention, allowing it to capture complex relationships within the input sequence.\n",
      "*   **Point-wise, fully connected layers:** After the self-attention layers, the model uses feed-forward networks applied to each position in the sequence independently.\n",
      "\n",
      "Therefore, the main architecture is the Transformer, characterized by its reliance on stacked self-attention and point-wise fully connected layers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Final Result\n",
    "print(ai_result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf19bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
